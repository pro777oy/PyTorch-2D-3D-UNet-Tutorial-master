{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0e296-afd2-44fb-986a-6523216d6f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from neptunecontrib.api import log_table\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from customdatasets import SegmentationDataSet4\n",
    "from transformations import (\n",
    "    ComposeDouble,\n",
    "    AlbuSeg2d,\n",
    "    FunctionWrapperDouble,\n",
    "    create_dense_target,\n",
    "    normalize_01,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a612f-24e4-4e05-b7ac-5bdaf29ecda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "checkpoint_location = r\"<your/location/here>\"  # where checkpoints are saved to\n",
    "project_name = (\n",
    "    \"<username>/<project>\"  # the project has to be created beforehand in neptune!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211db3c7-8c8e-4095-90ce-2efcffe39e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "params = {\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"DEPTH\": 4,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"NORMALIZATION\": \"group8\",\n",
    "    \"UPSAMPLING\": \"transposed\",\n",
    "    \"LR\": 0.0001,\n",
    "    \"WEIGTH_CE\": torch.tensor((0.2, 0.8)),\n",
    "    \"WEIGTH_DICE\": torch.tensor((0.0, 1.0)),\n",
    "    \"PRECISION\": 32,\n",
    "    \"LR_FINDER\": False,\n",
    "    \"INPUT_SIZE\": (128, 128),\n",
    "    \"CLASSES\": 2,\n",
    "    \"SEED\": 42,\n",
    "    \"EXPERIMENT\": \"carvana\",\n",
    "    \"MAXEPOCHS\": 10,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91a47a-a8cd-4cf6-85da-faaffd73c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api key\n",
    "api_key = os.environ['NEPTUNE']  # if this throws an error, you didn't set your env var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bb9984-c2a0-4c19-8997-2dcaf122979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root directory\n",
    "root = pathlib.Path.cwd() / \"Carvana\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978dee9-f39a-48d0-8596-e29a13b932f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get file paths\n",
    "def get_filenames_of_path(path: pathlib.Path, ext: str = \"*\"):\n",
    "    \"\"\"Returns a list of files in a directory/path. Uses pathlib.\"\"\"\n",
    "    filenames = [file for file in path.glob(ext) if file.is_file()]\n",
    "    return filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d0384e-d6c8-46a4-be3c-c476e1a9ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and target files\n",
    "inputs = get_filenames_of_path(root / \"Input\")\n",
    "targets = get_filenames_of_path(root / \"Target\")\n",
    "\n",
    "inputs.sort()\n",
    "targets.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca6882-2a10-4b8c-862a-0f7757a61ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-transformations\n",
    "pre_transforms = ComposeDouble(\n",
    "    [\n",
    "        FunctionWrapperDouble(\n",
    "            resize, input=True, target=False, output_shape=(128, 128, 3)\n",
    "        ),\n",
    "        FunctionWrapperDouble(\n",
    "            resize,\n",
    "            input=False,\n",
    "            target=True,\n",
    "            output_shape=(128, 128),\n",
    "            order=0,\n",
    "            anti_aliasing=False,\n",
    "            preserve_range=True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# training transformations and augmentations\n",
    "transforms_training = ComposeDouble(\n",
    "    [\n",
    "        AlbuSeg2d(albumentations.HorizontalFlip(p=0.5)),\n",
    "        FunctionWrapperDouble(create_dense_target, input=False, target=True),\n",
    "        FunctionWrapperDouble(\n",
    "            np.moveaxis, input=True, target=False, source=-1, destination=0\n",
    "        ),\n",
    "        FunctionWrapperDouble(normalize_01),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# validation transformations\n",
    "transforms_validation = ComposeDouble(\n",
    "    [\n",
    "        FunctionWrapperDouble(create_dense_target, input=False, target=True),\n",
    "        FunctionWrapperDouble(\n",
    "            np.moveaxis, input=True, target=False, source=-1, destination=0\n",
    "        ),\n",
    "        FunctionWrapperDouble(normalize_01),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# test transformations\n",
    "transforms_test = ComposeDouble(\n",
    "    [\n",
    "        FunctionWrapperDouble(create_dense_target, input=False, target=True),\n",
    "        FunctionWrapperDouble(\n",
    "            np.moveaxis, input=True, target=False, source=-1, destination=0\n",
    "        ),\n",
    "        FunctionWrapperDouble(normalize_01),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c45cd-9d0b-41e3-842f-379d557f47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training set and validation set (manually)\n",
    "inputs_train, inputs_valid, inputs_test = inputs[:80], inputs[80:], inputs[80:]\n",
    "targets_train, targets_valid, targets_test = targets[:80], targets[80:], targets[80:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a460b00-db69-4cb1-bb38-95c5df610ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "seed_everything(params[\"SEED\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf739f4-d58d-47be-aeb3-5f5335e74191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset training\n",
    "dataset_train = SegmentationDataSet4(\n",
    "    inputs=inputs_train,\n",
    "    targets=targets_train,\n",
    "    transform=transforms_training,\n",
    "    use_cache=True,\n",
    "    pre_transform=pre_transforms,\n",
    ")\n",
    "\n",
    "# dataset validation\n",
    "dataset_valid = SegmentationDataSet4(\n",
    "    inputs=inputs_valid,\n",
    "    targets=targets_valid,\n",
    "    transform=transforms_validation,\n",
    "    use_cache=True,\n",
    "    pre_transform=pre_transforms,\n",
    ")\n",
    "\n",
    "# dataset test\n",
    "dataset_test = SegmentationDataSet4(\n",
    "    inputs=inputs_test,\n",
    "    targets=targets_test,\n",
    "    transform=transforms_test,\n",
    "    use_cache=True,\n",
    "    pre_transform=pre_transforms,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112db55-dcbb-4ecb-9c02-a129f71c485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader training\n",
    "dataloader_training = DataLoader(\n",
    "    dataset=dataset_train, batch_size=params[\"BATCH_SIZE\"], shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "# dataloader validation\n",
    "dataloader_validation = DataLoader(\n",
    "    dataset=dataset_valid, batch_size=params[\"BATCH_SIZE\"], shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "# dataloader test\n",
    "dataloader_test = DataLoader(\n",
    "    dataset=dataset_test,\n",
    "    batch_size=1,  # has to be 1 for the analysis function at the bottom (k-highest, k-lowest)\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b8ac5-47e9-4904-ab93-2c77b0470ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using a the SegmentationDataSet4 here which returns a dict instead of a tuple\n",
    "batch = dataset_train[0]\n",
    "x, y, x_name, y_name = batch[\"x\"], batch[\"y\"], batch[\"x_name\"], batch[\"y_name\"]\n",
    "print(x.shape)\n",
    "print(x.min(), x.max())\n",
    "print(y.shape)\n",
    "print(torch.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9701b-e6da-46cd-acce-83a84c391280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little workaround to make the DatasetViewer work with the SegmentationDataSet4\n",
    "from visual import DatasetViewer\n",
    "\n",
    "\n",
    "class DatasetViewerExtra(DatasetViewer):\n",
    "    def show_sample(self):\n",
    "\n",
    "        # Get a sample from the dataset\n",
    "        sample = self.get_sample_dataset(self.index)\n",
    "        x, y, x_name, y_name = (\n",
    "            sample[\"x\"],\n",
    "            sample[\"y\"],\n",
    "            sample[\"x_name\"],\n",
    "            sample[\"y_name\"],\n",
    "        )\n",
    "\n",
    "        # Transform the sample to numpy, cpu and correct format to visualize\n",
    "        x = self.transform_x(x)\n",
    "        y = self.transform_y(y)\n",
    "\n",
    "        # Create or update image layer\n",
    "        if self.image_layer not in self.viewer.layers:\n",
    "            self.image_layer = self.create_image_layer(x, x_name)\n",
    "        else:\n",
    "            self.update_image_layer(self.image_layer, x, x_name)\n",
    "\n",
    "        # Create or update label layer\n",
    "        if self.label_layer not in self.viewer.layers:\n",
    "            self.label_layer = self.create_label_layer(y, y_name)\n",
    "        else:\n",
    "            self.update_label_layer(self.label_layer, y, y_name)\n",
    "\n",
    "        # Reset view\n",
    "        self.viewer.reset_view()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf07ac-568f-4326-a6cf-1e5bf61cee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DatasetViewerExtra instances\n",
    "dataset_viewer_training = DatasetViewerExtra(dataset_train)\n",
    "dataset_viewer_validation = DatasetViewerExtra(dataset_valid)\n",
    "dataset_viewer_test = DatasetViewerExtra(dataset_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514d386-fc66-46b5-bfc3-63899e8d7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open napari instance for training dataset\n",
    "# navigate with 'n' for next and 'b' for back on the keyboard\n",
    "dataset_viewer_training.napari()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c00de1-7187-4b44-b6e0-478b9e71c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open napari instance for validation dataset\n",
    "# navigate with 'n' for next and 'b' for back on the keyboard\n",
    "dataset_viewer_validation.napari()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f317893-c055-4964-98c6-624997f36d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open napari instance for test dataset\n",
    "# navigate with 'n' for next and 'b' for back on the keyboard\n",
    "dataset_viewer_test.napari()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a76e7e-1aae-4502-a663-e2ac2773dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neptune logger\n",
    "from pytorch_lightning.loggers.neptune import NeptuneLogger\n",
    "\n",
    "neptune_logger = NeptuneLogger(\n",
    "    api_key=api_key,\n",
    "    project_name=project_name,  # make sure this path exists in your netpune account\n",
    "    experiment_name=params[\n",
    "        \"EXPERIMENT\"\n",
    "    ],  # make sure this path exists in your netpune account\n",
    "    offline_mode=False,\n",
    "    params=params,\n",
    ")\n",
    "assert neptune_logger.name  # http GET request to check if the project exists\n",
    "\n",
    "# this can be a simple csv logger, or a custom logger\n",
    "# you can also ignore this, see the trainer class for more information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca8e611-ac9c-4de3-815f-dbdd8495a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning module\n",
    "from unet_lightning import Segmentation_UNET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1837a8f8-9440-49b0-bbc2-61a8522d3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "from unet import UNet\n",
    "\n",
    "model = UNet(\n",
    "    in_channels=3,\n",
    "    out_channels=2,\n",
    "    n_blocks=params[\"DEPTH\"],\n",
    "    start_filters=32,\n",
    "    activation=params[\"ACTIVATION\"],\n",
    "    normalization=params[\"NORMALIZATION\"],\n",
    "    conv_mode=\"same\",\n",
    "    dim=2,\n",
    "    up_mode=params[\"UPSAMPLING\"],\n",
    ")\n",
    "\n",
    "# you can replace this model with any other segmentation model here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6bbdf1-9b7f-48a4-90b1-527f41e8100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task init\n",
    "task = Segmentation_UNET(\n",
    "    model,\n",
    "    lr=params[\"LR\"],\n",
    "    weight_ce=params[\"WEIGTH_CE\"],\n",
    "    weight_dice=params[\"WEIGTH_DICE\"],\n",
    "    num_classes=params[\"CLASSES\"],\n",
    "    metrics=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7279df4-c139-4047-83bd-796c237931bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    "    EarlyStopping,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"checkpoint_valid_f1_epoch\", mode=\"max\")\n",
    "learningrate_callback = LearningRateMonitor(logging_interval=\"step\", log_momentum=False)\n",
    "# early_stopping_callback = EarlyStopping(monitor=\"checkpoint_valid_f1_epoch\", patience=10, mode=\"max\") # throws an error atm, because of the custom metric computation that I use -> it's better to use their approach\n",
    "\n",
    "# 3 very basic but important callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d380fa5-fd48-4be9-84bf-d4a3ea433e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer init\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=1,\n",
    "    precision=params[\"PRECISION\"],  # try 16 with enable_pl_optimizer=False\n",
    "    benchmark=True,  # good if the input sizes do not change, will increase speed\n",
    "    callbacks=[checkpoint_callback, learningrate_callback],\n",
    "    default_root_dir=checkpoint_location,  # where checkpoints are saved to\n",
    "    logger=neptune_logger,  # you can also set it to False without breaking the code!\n",
    "    log_every_n_steps=1,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "# the trainer class has many parameters!\n",
    "# you can also set the logger arg to to False without breaking the code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a257ce4-b79c-4863-97b2-85cef4ecb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate finder\n",
    "if params[\"LR_FINDER\"]:\n",
    "    lr_finder = trainer.tuner.lr_find(\n",
    "        model=task,\n",
    "        train_dataloader=dataloader_training,\n",
    "        val_dataloaders=dataloader_validation,\n",
    "        min_lr=1e-8,\n",
    "        max_lr=1.0,\n",
    "        num_training=100,  # number of learning rates to test\n",
    "        mode=\"exponential\",\n",
    "        early_stop_threshold=None,\n",
    "    )\n",
    "\n",
    "    lr_finder_results = lr_finder.results  # results: lr vs loss in dict\n",
    "    fig = lr_finder.plot(suggest=True, show=True)  # show fig of suggested lr\n",
    "    neptune_logger.experiment.log_image(\n",
    "        \"Learning Rate Range Test\", fig\n",
    "    )  # log to neptune\n",
    "    new_lr = lr_finder.suggestion()  # new lr\n",
    "\n",
    "    task.lr = new_lr  # update with suggested lr\n",
    "    neptune_logger.experiment.set_property(\"LR\", new_lr)\n",
    "\n",
    "# PL has a learning rate finder, very convenient!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af4109-693e-43ff-88d1-b4ae318ff548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "trainer.max_epochs = params[\"MAXEPOCHS\"]\n",
    "trainer.fit(\n",
    "    task, train_dataloader=dataloader_training, val_dataloaders=dataloader_validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd8197-5ed9-4090-b1c3-345a16b5e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start testing\n",
    "trainer.test(ckpt_path=\"best\", test_dataloaders=dataloader_test)\n",
    "# this is how you would run your test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb852f9a-2930-4682-a3f7-62d93e18a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log packages\n",
    "import importlib_metadata\n",
    "\n",
    "dists = importlib_metadata.distributions()\n",
    "packages = {\n",
    "    idx: (dist.metadata[\"Name\"], dist.version) for idx, dist in enumerate(dists)\n",
    "}\n",
    "\n",
    "packages_df = pd.DataFrame.from_dict(\n",
    "    packages, orient=\"index\", columns=[\"package\", \"version\"]\n",
    ")\n",
    "\n",
    "log_table(name=\"packages\", table=packages_df, experiment=neptune_logger.experiment)\n",
    "\n",
    "packages_df\n",
    "# I like to log the packages of the env that I used for the training run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc672413-d36e-4221-8df3-902d85488891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # log checkpoint including the model weights\n",
    "# checkpoint_path = pathlib.Path(checkpoint_callback.best_model_path)\n",
    "# neptune_logger.experiment.set_property('checkpoint_name', checkpoint_path.name)\n",
    "# neptune_logger.experiment.log_artifact(str(checkpoint_path))\n",
    "\n",
    "# you can either upload the complete checkpoint here or extract the model weights and upload them to your neptune experiment. Let me know if you're interested in knowing how this can be done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ed79d-4e2b-4b23-a255-280e157776f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get k highest and lowest scores for analysis purposes, only works if batch size of the test dataset is set to 1\n",
    "def get_k_highest_values(scores, k):\n",
    "    # return indices\n",
    "    return np.argpartition(np.array(scores), -k)[-k:]\n",
    "\n",
    "\n",
    "def get_k_lowest_values(scores, k):\n",
    "    # return indices\n",
    "    return np.argpartition(np.array(scores), k)[:k]\n",
    "\n",
    "\n",
    "def log_k_worst_best_scores(metric_obj, k):\n",
    "    import itertools\n",
    "\n",
    "    scores = metric_obj.get_metrics_epoch(last=True, transpose=False).numpy()\n",
    "    names = np.array(list(itertools.chain.from_iterable(metric_obj.last_names)))\n",
    "\n",
    "    k_lowest = get_k_lowest_values(scores, k=k)  # returns indices\n",
    "    k_highest = get_k_highest_values(scores, k=k)  # returns indices\n",
    "\n",
    "    df_lowest = pd.DataFrame(\n",
    "        {f\"{metric_obj}\": scores[k_lowest], \"name\": names[k_lowest]}\n",
    "    )\n",
    "    df_highest = pd.DataFrame(\n",
    "        {f\"{metric_obj}\": scores[k_highest], \"name\": names[k_highest]}\n",
    "    )\n",
    "\n",
    "    log_table(\n",
    "        name=f\"{metric_obj}-lowest\",\n",
    "        table=df_lowest,\n",
    "        experiment=neptune_logger.experiment,\n",
    "    )\n",
    "    log_table(\n",
    "        name=f\"{metric_obj}-highest\",\n",
    "        table=df_highest,\n",
    "        experiment=neptune_logger.experiment,\n",
    "    )\n",
    "\n",
    "    return df_lowest, df_highest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7209285-fb2b-41b4-bdaf-d0c07a178bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_k_worst_best_scores(task.f1_test, k=5)\n",
    "log_k_worst_best_scores(task.iou_test, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210349c-628b-48e3-a725-8393307e465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop experiment\n",
    "neptune_logger.experiment.stop()\n",
    "neptune_logger.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e1165-fd27-4029-87ba-d2b81197b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse your results in neptune.ai!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
